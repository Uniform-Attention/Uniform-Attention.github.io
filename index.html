<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Scratching Visual Transformer's Back with Uniform Attention">
  <meta name="keywords" content="Transformer, Attention, Analysis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Scratching Visual Transformer's Back with Uniform Attention</title>

  <!-- fontawesome for icons -->
  <script src="https://kit.fontawesome.com/b6f2d55583.js" crossorigin="anonymous"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Scratching Visual Transformer's Back <br> with Uniform Attention</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/view/namhyeonwoo/">Nam Hyeon-Woo</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://ug-kim.notion.site">Kim Yu-Ji</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
            </span>
            <span class="author-block">
              <a href="https://coallaoh.github.io/">Seong Joon Oh</a>,
            </span>
            <span class="author-block">
              <a href="https://ami.postech.ac.kr/members/tae-hyun-oh">Tae-Hyun Oh</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Naver AI Lab, POSTECH, University of Tubingen</span>
          </div>
          <div class="column has-text-centered">
            <span class="author-block">International Conference on Computer Vision (ICCV) 2023</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2210.08457"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2210.08457"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-brands fa-github"></i>
                  </span>
                  <span>GitHub</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <p class="columns is-centered has-text-centered"><img src="images/teaser.png" width="60%"></p>
      <div class="column is-four-fifths">
        
      </div>
      <h2 class="subtitle has-text-centered">
        Dense attention is hard to learn by softmax. Infusing dense attention splits the responsibility of interactions;
        the burden of interactions of self-attention is reduced. 
        Self-attention is now more likely to learn sparse interaction that is in favor of softmax.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          The favorable performance of Vision Transformers
          (ViTs) is often attributed to the multi-head self-attention
          (MSA), which enables global interactions at each layer of
          a ViT model. Previous works acknowledge the property of
          long-range dependency for the effectiveness in MSA. 
        </p>
          In this
          work, we study the role of MSA in terms of the different axis,
          density. Our preliminary analyses suggest that the spatial
          interactions of learned attention maps are close to dense
          interactions rather than sparse ones. This is a curious phenomenon
          because dense attention maps are harder for the
          model to learn due to softmax. We interpret this opposite
          behavior against softmax as a strong preference for
          the ViT models to include dense interaction. We thus manually
          insert the dense uniform attention to each layer of the
          ViT models to supply the much-needed dense interactions.
          We call this method Context Broadcasting, CB. 
        </p>
          Our study demonstrates the inclusion of CB takes the role of dense 
          attention, and thereby reduces the degree of density in the
          original attention maps by complying softmax in MSA. We
          also show that, with negligible costs of CB (1 line in your
          model code and no additional parameters), both the capacity
          and generalizability of the ViT models are increased.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="two-columns-grid">
          <div><img src="images/entropy.png"></div>
          <div><img src="images/softmax.png"></div>
        </div>
        <div align="left">
          <ul class="bulleted-list">
            <li style="list-style-type:disc">A majority of the attention in ViTs has such high entropy values</li>
          </ul>
          <ul class="bulleted-list">
            <li style="list-style-type:disc">The steeper gradient for the MSA layer with denser attention maps</li>
          </ul>
          <ul class="bulleted-list">
            <li style="list-style-type:disc">Hard to learn Dense attention maps, but vital to ViTs</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <img src="images/uniform.png" width="60%">
        <div align="left">
          <p> Decide to inject uniform attention because </p>
            <div class="indented">
              <p >&nbsp;&nbsp;&nbsp;&nbsp;(1) uniform attention is the densest attention and is unstable in terms of gradient view</p>
              <p class="">&nbsp;&nbsp;&nbsp;&nbsp;(2) but, humans can supply uniform attention easily</p>
              <p class="">&nbsp;&nbsp;&nbsp;&nbsp;(3) uniform attention requires no additional parameters and small computation costs. </p>
            </div>
            <p class="">We do this through the broadcasting context with the CB module.</p>
        </div>
      </div>
    </div>
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Characteristics</h2>
        <div align="left">
          <ul class="bulleted-list">
            <li style="list-style-type:disc">The insertion of CB module lowers the entropy values significantly</li></ul>
          <ul class="bulleted-list">
            <li style="list-style-type:disc">Injecting the dense global interactions into ViT does not hurt the range of interactions</li></ul>
          <ul class="bulleted-list">
            <li style="list-style-type:disc">The upper layers prefer dense interactions more than the lower layers</li></ul>
          <ul class="bulleted-list">
            <li style="list-style-type:disc">More effective in a lower number of heads rather than the large number of heads</li></ul>
        </div>
    </div>
  </div>

  

</div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
    @inproceedings{hyeon2022scratching,
      title={Scratching Visual Transformer's Back with Uniform Attention},
      author={Hyeon-Woo, Nam and Yu-Ji, Kim and Heo, Byeongho and Han, Doonyoon and Oh, Seong Joon and Oh, Tae-Hyun},
      booktitle = {ICCV},
      year={2023}
    }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <h2 class="title is-3">Acknowledgement</h2>
          <p>
            This work was partly supported 
            by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) 
            (No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities),
            (No.2022-0-00290, Visual Intelligence for Space-Time Understanding and Generation based on Multi-layered Visual Common Sense)
            (No. 2020-0-00004, Development of Previsional Intelligence based on Long-term Visual Memory Network).
          </p>

          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to the <a href="https://github.com/nerfies/nerfies.github.io">original page</a> in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
